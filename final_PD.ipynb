{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    set_seed,\n",
    "    DataCollator\n",
    ")\n",
    "from typing import Callable, Dict, Optional, List\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "set_seed(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DIR_NAME = 'parapragh_decomposition'\n",
    "PRETRAINED_MODEL = 'bert-base-uncased'\n",
    "NUM_LABELS = 2\n",
    "PAD_MAX_LEN = 65\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCH = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/tidarren1020/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/tidarren1020/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/tidarren1020/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_jokes_path = './RedditHumorDetection/full_datasets/short_jokes/data/shortjokes.csv'\n",
    "# short_jokes_train_path = './RedditHumorDetection/data/short_jokes/train.tsv'\n",
    "# short_jokes_test_path = './RedditHumorDetection/data/short_jokes/test.tsv'\n",
    "# short_jokes_dev_path = './RedditHumorDetection/data/short_jokes/dev.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "puns_train_path = './RedditHumorDetection/data/puns/train.tsv'\n",
    "puns_test_path = './RedditHumorDetection/data/puns/test.tsv'\n",
    "puns_dev_path = './RedditHumorDetection/data/puns/dev.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Paragraph Decmposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path, header=None, names=['id','label','a','text'])\n",
    "    df = df[['label','text']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_decmposition(data_path):\n",
    "    df = load_data(data_path)\n",
    "    \n",
    "    dataAug = []\n",
    "    for _id,row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        for i in range(2,len(tokens)):\n",
    "            text_a = ' '.join(tokens[:i])\n",
    "            text_b = ' '.join(tokens[i:])\n",
    "\n",
    "            d  = {'text_a':text_a, 'text_b':text_b, 'label':label, 'origin_id':_id}\n",
    "\n",
    "            dataAug.append(d)\n",
    "    \n",
    "    df_dataAug = pd.DataFrame(dataAug)\n",
    "    print('=== Data Augmentation: paragraph decmposition ===')\n",
    "    print('[Before]')\n",
    "    print('# of label=0:',sum(df.label==0))\n",
    "    print('# of label=1:',sum(df.label==1))\n",
    "    print('\\n[After]')\n",
    "    print('# of label=0:',sum(df_dataAug.label==0))\n",
    "    print('# of label=1:',sum(df_dataAug.label==1))\n",
    "    print('=== end ===')\n",
    "    return df_dataAug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumorDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, df, tokenizer, mode='train'):\n",
    "        self.df = df\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        self.mode = mode\n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode=='train':\n",
    "            text_a = self.df.loc[idx, 'text_a']\n",
    "            text_b = self.df.loc[idx, 'text_b']\n",
    "            inputDict = tokenizer.encode_plus(text_a, text_b)\n",
    "        else:\n",
    "            text_a = self.df.loc[idx, 'text']\n",
    "            inputDict = tokenizer.encode_plus(text_a)\n",
    "        \n",
    "        label = self.df.loc[idx, 'label']\n",
    "        inputDict['label'] = label\n",
    "        \n",
    "        return inputDict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_batch_len, pad_value):\n",
    "    return seq + (max_batch_len - len(seq)) * [pad_value]\n",
    "\n",
    "class Collator(DataCollator):\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        batch_inputs = list()\n",
    "        batch_attention_masks = list()\n",
    "        batch_token_type_ids = list()\n",
    "        labels = list()\n",
    "        max_size = max([len(ex['input_ids']) for ex in batch])\n",
    "        for item in batch:\n",
    "            batch_inputs += [pad_seq(item['input_ids'], max_size, self.pad_token_id)]\n",
    "            batch_attention_masks += [pad_seq(item['attention_mask'], max_size, 0)]\n",
    "            batch_token_type_ids += [pad_seq(item['token_type_ids'], max_size, 0)]\n",
    "            labels.append(item['label'])\n",
    "\n",
    "        return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(batch_token_type_ids, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Augmentation: paragraph decmposition ===\n",
      "[Before]\n",
      "# of label=0: 1810\n",
      "# of label=1: 1809\n",
      "\n",
      "[After]\n",
      "# of label=0: 21403\n",
      "# of label=1: 20278\n",
      "=== end ===\n"
     ]
    }
   ],
   "source": [
    "df_puns_train_dataAug = paragraph_decmposition(puns_train_path)\n",
    "df_puns_dev = load_data(puns_dev_path)\n",
    "\n",
    "train_dataset = HumorDataset(df_puns_train_dataAug, tokenizer)\n",
    "eval_dataset = HumorDataset(df_puns_dev, tokenizer, mode='dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"acc\": (preds == p.label_ids).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_TRAINSET = len(train_dataset)\n",
    "LOGGING_STEPS = math.ceil(NUM_TRAINSET/BATCH_SIZE)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/{}\".format(SAVED_DIR_NAME),\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=MAX_EPOCH,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_first_step=True,\n",
    "    save_steps=LOGGING_STEPS,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    #learning_rate=2e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.training_args:PyTorch: setting up devices\n",
      "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=Collator(pad_token_id=tokenizer.pad_token_id),\n",
    "        compute_metrics=compute_metrics,\n",
    "        tb_writer=SummaryWriter(log_dir='logs', flush_secs=10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running training *****\n",
      "INFO:transformers.trainer:  Num examples = 41681\n",
      "INFO:transformers.trainer:  Num Epochs = 5\n",
      "INFO:transformers.trainer:  Instantaneous batch size per device = 128\n",
      "INFO:transformers.trainer:  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "INFO:transformers.trainer:  Gradient Accumulation steps = 1\n",
      "INFO:transformers.trainer:  Total optimization steps = 1630\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79cdd25d8334f04aead14cc878cab78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c9ad2d51ea420fbdf8958e4c5877c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=326, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.00234516449501178, \"learning_rate\": 4.996932515337424e-05, \"epoch\": 0.003067484662576687, \"step\": 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1452f35e9df348b38c9ae769a22b80bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.6520037651062012, \"eval_acc\": 0.6517412935323383, \"epoch\": 0.003067484662576687, \"step\": 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.08084941734816947, \"learning_rate\": 4e-05, \"epoch\": 1.0, \"step\": 326}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d892660dcf6540958d82fa6aee3f6b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/parapragh_decomposition/checkpoint-326\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/parapragh_decomposition/checkpoint-326/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.45084678530693056, \"eval_acc\": 0.9187396351575456, \"epoch\": 1.0, \"step\": 326}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/parapragh_decomposition/checkpoint-326/pytorch_model.bin\n",
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:201: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbf26f7d01a489fb6d7e9b94856f6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=326, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0023290376606817996, \"learning_rate\": 3e-05, \"epoch\": 2.0, \"step\": 652}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d655ccb905c44dab494a1e050df43c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/parapragh_decomposition/checkpoint-652\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/parapragh_decomposition/checkpoint-652/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.5679295063018799, \"eval_acc\": 0.9137645107794361, \"epoch\": 2.0, \"step\": 652}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/parapragh_decomposition/checkpoint-652/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94495b905348461693708f071968cb72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=326, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0008036018569584674, \"learning_rate\": 2e-05, \"epoch\": 3.0, \"step\": 978}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5396749a905e41648a0e2a5af38fb739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/parapragh_decomposition/checkpoint-978\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/parapragh_decomposition/checkpoint-978/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.6377912640571595, \"eval_acc\": 0.9170812603648425, \"epoch\": 3.0, \"step\": 978}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/parapragh_decomposition/checkpoint-978/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477e02fd427540968d361a773629f332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=326, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.00028497528941766933, \"learning_rate\": 1e-05, \"epoch\": 4.0, \"step\": 1304}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8677713a475484b87e557c2c8fcde4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/parapragh_decomposition/checkpoint-1304\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/parapragh_decomposition/checkpoint-1304/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.6359874367713928, \"eval_acc\": 0.9220563847429519, \"epoch\": 4.0, \"step\": 1304}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/parapragh_decomposition/checkpoint-1304/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5840028f5735405fa39ac669465a24c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=326, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 9.977922872741966e-05, \"learning_rate\": 0.0, \"epoch\": 5.0, \"step\": 1630}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0ed6d775204bceafb3e5b93f162515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/parapragh_decomposition/checkpoint-1630\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/parapragh_decomposition/checkpoint-1630/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.6318127870559692, \"eval_acc\": 0.9220563847429519, \"epoch\": 5.0, \"step\": 1630}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/parapragh_decomposition/checkpoint-1630/pytorch_model.bin\n",
      "INFO:transformers.trainer:\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1630, training_loss=0.01734239517579332)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4fba0c061847adab06ac1527fe8dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"eval_loss\": 0.6318127870559692, \"eval_acc\": 0.9220563847429519, \"epoch\": 5.0, \"step\": 1630}\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find out Punchline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_inputs = list()\n",
    "    batch_attention_masks = list()\n",
    "    batch_token_type_ids = list()\n",
    "    labels = list()\n",
    "    max_size = max([len(ex['input_ids']) for ex in batch])\n",
    "    for item in batch:\n",
    "        batch_inputs += [pad_seq(item['input_ids'], max_size, 0)]\n",
    "        batch_attention_masks += [pad_seq(item['attention_mask'], max_size, 0)]\n",
    "        batch_token_type_ids += [pad_seq(item['token_type_ids'], max_size, 0)]\n",
    "        labels.append(item['label'])\n",
    "\n",
    "    return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(batch_token_type_ids, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punchline(model, dataloader, compute_acc=False):\n",
    "    sm = torch.nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = {key:val.to(\"cuda:0\") for key,val in data.items() if val is not None}\n",
    "            tokens_tensors = data['input_ids']\n",
    "            segments_tensors = data['token_type_ids']\n",
    "            masks_tensors = data['attention_mask']\n",
    "            labels = data['labels']\n",
    "            \n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "#             print(logits)\n",
    "            prob = sm(logits)\n",
    "#             print(prob)\n",
    "            prob = list(map(lambda x:x[1], prob))\n",
    "            pred = torch.tensor(prob)\n",
    "            pred = torch.argmax(pred)\n",
    "            \n",
    "    return int(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2dict(_id):\n",
    "    df_tmp = df_puns_train_dataAug[df_puns_train_dataAug.origin_id==_id].reset_index(drop=True)\n",
    "    dataset_tmp = HumorDataset(df_tmp, tokenizer)\n",
    "    if len(dataset_tmp)==0:\n",
    "        print('batchsize is 0',_id)\n",
    "    dataloader = DataLoader(dataset_tmp, batch_size=len(dataset_tmp), \n",
    "                             collate_fn=collate_fn)\n",
    "    \n",
    "    punchline_idx = get_punchline(model, dataloader, compute_acc=True)\n",
    "    \n",
    "    label = df_tmp.loc[punchline_idx, 'label']\n",
    "    text_a = df_tmp.loc[punchline_idx, 'text_a']\n",
    "    text_b = df_tmp.loc[punchline_idx, 'text_b']\n",
    "    origin_id = df_tmp.loc[punchline_idx, 'origin_id']\n",
    "    \n",
    "    dataDict = {'label':label, 'text_a':text_a, 'text_b':text_b, \n",
    "                'origin_id':origin_id, 'punchline_idx':punchline_idx}\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.4 s, sys: 1.12 s, total: 49.6 s\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "data_punchline = []\n",
    "\n",
    "for _id in range(len(df_puns_train)):\n",
    "    dataDict = id2dict(_id)\n",
    "    data_punchline.append(dataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3619 entries, 0 to 3618\n",
      "Data columns (total 5 columns):\n",
      "label            3619 non-null int64\n",
      "origin_id        3619 non-null int64\n",
      "punchline_idx    3619 non-null int64\n",
      "text_a           3619 non-null object\n",
      "text_b           3619 non-null object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 141.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_puns_punchline = pd.DataFrame(data_punchline)\n",
    "df_puns_punchline.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>origin_id</th>\n",
       "      <th>punchline_idx</th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>I m hoping they ll come and see this</td>\n",
       "      <td>and say We have to have this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>a man who cannot read the sign that warns peop...</td>\n",
       "      <td>illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>i fired the floor refinishers they simply coul...</td>\n",
       "      <td>their lacquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>an elevator makes ghosts happy because it lifts</td>\n",
       "      <td>the spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>the first drinking establishment in alaska</td>\n",
       "      <td>was a polar bar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  origin_id  punchline_idx  \\\n",
       "0      0          0              7   \n",
       "1      1          1             15   \n",
       "2      1          2              8   \n",
       "3      1          3              6   \n",
       "4      1          4              4   \n",
       "\n",
       "                                              text_a  \\\n",
       "0               I m hoping they ll come and see this   \n",
       "1  a man who cannot read the sign that warns peop...   \n",
       "2  i fired the floor refinishers they simply coul...   \n",
       "3    an elevator makes ghosts happy because it lifts   \n",
       "4         the first drinking establishment in alaska   \n",
       "\n",
       "                         text_b  \n",
       "0  and say We have to have this  \n",
       "1                    illiterate  \n",
       "2                 their lacquer  \n",
       "3                   the spirits  \n",
       "4               was a polar bar  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_puns_punchline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puns_punchline.to_csv('df_puns_punchline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puns_train_dataAug.to_csv('df_puns_pd.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
