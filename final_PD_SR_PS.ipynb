{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    set_seed,\n",
    "    DataCollator\n",
    ")\n",
    "from typing import Callable, Dict, Optional, List\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "set_seed(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DIR_NAME = 'DP_SR_PS'\n",
    "PRETRAINED_MODEL = 'bert-base-uncased'\n",
    "NUM_LABELS = 2\n",
    "PAD_MAX_LEN = 65\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCH = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/tidarren1020/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/tidarren1020/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/tidarren1020/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_jokes_path = './RedditHumorDetection/full_datasets/short_jokes/data/shortjokes.csv'\n",
    "# short_jokes_train_path = './RedditHumorDetection/data/short_jokes/train.tsv'\n",
    "# short_jokes_test_path = './RedditHumorDetection/data/short_jokes/test.tsv'\n",
    "# short_jokes_dev_path = './RedditHumorDetection/data/short_jokes/dev.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "puns_train_path = './RedditHumorDetection/data/puns/train.tsv'\n",
    "puns_test_path = './RedditHumorDetection/data/puns/test.tsv'\n",
    "puns_dev_path = './RedditHumorDetection/data/puns/dev.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Paragraph Decmposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path, header=None, names=['id','label','a','text'])\n",
    "    df = df[['label','text']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_decmposition(data_path):\n",
    "    df = load_data(data_path)\n",
    "    \n",
    "    dataAug = []\n",
    "    for _id,row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        for i in range(2,len(tokens)):\n",
    "            text_a = ' '.join(tokens[:i])\n",
    "            text_b = ' '.join(tokens[i:])\n",
    "\n",
    "            d  = {'text_a':text_a, 'text_b':text_b, 'label':label, 'origin_id':_id}\n",
    "\n",
    "            dataAug.append(d)\n",
    "    \n",
    "    df_dataAug = pd.DataFrame(dataAug)\n",
    "    print('=== Data Augmentation: paragraph decmposition ===')\n",
    "    print('[Before]')\n",
    "    print('# of label=0:',sum(df.label==0))\n",
    "    print('# of label=1:',sum(df.label==1))\n",
    "    print('\\n[After]')\n",
    "    print('# of label=0:',sum(df_dataAug.label==0))\n",
    "    print('# of label=1:',sum(df_dataAug.label==1))\n",
    "    print('=== end ===')\n",
    "    return df_dataAug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumorDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, df, tokenizer, mode='train'):\n",
    "        self.df = df\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        self.mode = mode\n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode=='train':\n",
    "            text_a = self.df.loc[idx, 'text_a']\n",
    "            text_b = self.df.loc[idx, 'text_b']\n",
    "            inputDict = tokenizer.encode_plus(text_a, text_b)\n",
    "        else:\n",
    "            text_a = self.df.loc[idx, 'text']\n",
    "            inputDict = tokenizer.encode_plus(text_a)\n",
    "        \n",
    "        label = self.df.loc[idx, 'label']\n",
    "        inputDict['label'] = label\n",
    "        \n",
    "        return inputDict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_batch_len, pad_value):\n",
    "    return seq + (max_batch_len - len(seq)) * [pad_value]\n",
    "\n",
    "class Collator(DataCollator):\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        batch_inputs = list()\n",
    "        batch_attention_masks = list()\n",
    "        batch_token_type_ids = list()\n",
    "        labels = list()\n",
    "        max_size = max([len(ex['input_ids']) for ex in batch])\n",
    "        for item in batch:\n",
    "            batch_inputs += [pad_seq(item['input_ids'], max_size, self.pad_token_id)]\n",
    "            batch_attention_masks += [pad_seq(item['attention_mask'], max_size, 0)]\n",
    "            batch_token_type_ids += [pad_seq(item['token_type_ids'], max_size, 0)]\n",
    "            labels.append(item['label'])\n",
    "\n",
    "        return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(batch_token_type_ids, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "from nlpaug.util import Action\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'an elevator makes ghosts happy because it lifts'\n",
    "\n",
    "# aug = naw.SynonymAug(aug_src='wordnet')\n",
    "# augmented_text = aug.augment(text, n=5)\n",
    "# print(\"Original:\")\n",
    "# print(text)\n",
    "# print(\"Augmented Text:\")\n",
    "# for t in augmented_text:\n",
    "#     print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement():\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    data_synonym_replacement = []\n",
    "    df_puns_punchline = pd.read_csv('df_puns_punchline.csv')\n",
    "    for i,row in df_puns_punchline.iterrows():\n",
    "        text_a = row['text_a']\n",
    "        augmented_text = aug.augment(text_a, n=5)\n",
    "        d = {k:v for k,v in row.items()}\n",
    "        for text in augmented_text:\n",
    "            d_tmp = d.copy()\n",
    "            d_tmp['text_a'] = text\n",
    "            data_synonym_replacement.append(d_tmp)\n",
    "    df = pd.DataFrame(data_synonym_replacement)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17153 entries, 0 to 17152\n",
      "Data columns (total 5 columns):\n",
      "label            17153 non-null int64\n",
      "origin_id        17153 non-null int64\n",
      "punchline_idx    17153 non-null int64\n",
      "text_a           17153 non-null object\n",
      "text_b           17153 non-null object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 670.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_puns_sr = pd.read_csv('df_puns_sr.csv')\n",
    "df_puns_sr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>origin_id</th>\n",
       "      <th>punchline_idx</th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>I m hoping they ll come and see this</td>\n",
       "      <td>and say We have to have this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>a man who cannot read the sign that warns peop...</td>\n",
       "      <td>illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>i fired the floor refinishers they simply coul...</td>\n",
       "      <td>their lacquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>an elevator makes ghosts happy because it lifts</td>\n",
       "      <td>the spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>the first drinking establishment in alaska</td>\n",
       "      <td>was a polar bar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  origin_id  punchline_idx  \\\n",
       "0      0          0              7   \n",
       "1      1          1             15   \n",
       "2      1          2              8   \n",
       "3      1          3              6   \n",
       "4      1          4              4   \n",
       "\n",
       "                                              text_a  \\\n",
       "0               I m hoping they ll come and see this   \n",
       "1  a man who cannot read the sign that warns peop...   \n",
       "2  i fired the floor refinishers they simply coul...   \n",
       "3    an elevator makes ghosts happy because it lifts   \n",
       "4         the first drinking establishment in alaska   \n",
       "\n",
       "                         text_b  \n",
       "0  and say We have to have this  \n",
       "1                    illiterate  \n",
       "2                 their lacquer  \n",
       "3                   the spirits  \n",
       "4               was a polar bar  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_puns_punchline = pd.read_csv('df_puns_punchline.csv')\n",
    "df_puns_punchline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>origin_id</th>\n",
       "      <th>punchline_idx</th>\n",
       "      <th>text_b</th>\n",
       "      <th>text_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>I m hoping they ll come and see this</td>\n",
       "      <td>and say We have to have this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>a man who cannot read the sign that warns peop...</td>\n",
       "      <td>illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>i fired the floor refinishers they simply coul...</td>\n",
       "      <td>their lacquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>an elevator makes ghosts happy because it lifts</td>\n",
       "      <td>the spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>the first drinking establishment in alaska</td>\n",
       "      <td>was a polar bar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  origin_id  punchline_idx  \\\n",
       "0      0          0              7   \n",
       "1      1          1             15   \n",
       "2      1          2              8   \n",
       "3      1          3              6   \n",
       "4      1          4              4   \n",
       "\n",
       "                                              text_b  \\\n",
       "0               I m hoping they ll come and see this   \n",
       "1  a man who cannot read the sign that warns peop...   \n",
       "2  i fired the floor refinishers they simply coul...   \n",
       "3    an elevator makes ghosts happy because it lifts   \n",
       "4         the first drinking establishment in alaska   \n",
       "\n",
       "                         text_a  \n",
       "0  and say We have to have this  \n",
       "1                    illiterate  \n",
       "2                 their lacquer  \n",
       "3                   the spirits  \n",
       "4               was a polar bar  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_puns_ps = df_puns_punchline.rename(columns={'text_a':'text_b', 'text_b':'text_a'})\n",
    "df_puns_ps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3619 entries, 0 to 3618\n",
      "Data columns (total 5 columns):\n",
      "label            3619 non-null int64\n",
      "origin_id        3619 non-null int64\n",
      "punchline_idx    3619 non-null int64\n",
      "text_b           3619 non-null object\n",
      "text_a           3619 non-null object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 141.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_puns_ps.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41681 entries, 0 to 41680\n",
      "Data columns (total 4 columns):\n",
      "label        41681 non-null int64\n",
      "origin_id    41681 non-null int64\n",
      "text_a       41681 non-null object\n",
      "text_b       41681 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_puns_pd = pd.read_csv('df_puns_pd.csv')\n",
    "df_puns_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Paragraph Decomposition + Synonym Replacement + Paragraph Swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62453 entries, 0 to 62452\n",
      "Data columns (total 5 columns):\n",
      "label            62453 non-null int64\n",
      "origin_id        62453 non-null int64\n",
      "text_a           62453 non-null object\n",
      "text_b           62453 non-null object\n",
      "punchline_idx    20772 non-null float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_concat = pd.concat([df_puns_pd, df_puns_sr, df_puns_ps], sort=False).reset_index(drop=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puns_dev = load_data(puns_dev_path)\n",
    "\n",
    "train_dataset = HumorDataset(df_concat, tokenizer)\n",
    "eval_dataset = HumorDataset(df_puns_dev, tokenizer, mode='dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"acc\": (preds == p.label_ids).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_TRAINSET = len(train_dataset)\n",
    "LOGGING_STEPS = math.ceil(NUM_TRAINSET/BATCH_SIZE)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/{}\".format(SAVED_DIR_NAME),\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=MAX_EPOCH,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_first_step=True,\n",
    "    save_steps=LOGGING_STEPS,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    #learning_rate=2e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.training_args:PyTorch: setting up devices\n",
      "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=Collator(pad_token_id=tokenizer.pad_token_id),\n",
    "        compute_metrics=compute_metrics,\n",
    "        tb_writer=SummaryWriter(log_dir='logs', flush_secs=10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running training *****\n",
      "INFO:transformers.trainer:  Num examples = 62453\n",
      "INFO:transformers.trainer:  Num Epochs = 5\n",
      "INFO:transformers.trainer:  Instantaneous batch size per device = 128\n",
      "INFO:transformers.trainer:  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "INFO:transformers.trainer:  Gradient Accumulation steps = 1\n",
      "INFO:transformers.trainer:  Total optimization steps = 2440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db76124f9094675b6a704655196c930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a302d657f9d4be18246f6bf15c5dabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=488, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0014859932612200253, \"learning_rate\": 4.9979508196721315e-05, \"epoch\": 0.0020491803278688526, \"step\": 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6b07290204453081a4ea2fef478074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.6542993783950806, \"eval_acc\": 0.6301824212271974, \"epoch\": 0.0020491803278688526, \"step\": 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.07936141640806693, \"learning_rate\": 4e-05, \"epoch\": 1.0, \"step\": 488}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bb5e60c9ef4d90820ced38c4be4292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-488\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-488/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.3907766819000244, \"eval_acc\": 0.9170812603648425, \"epoch\": 1.0, \"step\": 488}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-488/pytorch_model.bin\n",
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:201: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5729aed37f6148c79b7c4d179b3813cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=488, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.005241578853216555, \"learning_rate\": 3e-05, \"epoch\": 2.0, \"step\": 976}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c434409f8aae430a889a501eaf084beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-976\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-976/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.4534082353115082, \"eval_acc\": 0.9237147595356551, \"epoch\": 2.0, \"step\": 976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-976/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5930463ff64422d9a10213233af2851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=488, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0017128793091406344, \"learning_rate\": 2e-05, \"epoch\": 3.0, \"step\": 1464}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4fed31fb0334d32aa14c49330c20755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-1464\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-1464/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.5465590476989746, \"eval_acc\": 0.9187396351575456, \"epoch\": 3.0, \"step\": 1464}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-1464/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2377cf9e982047a0a55b0b23b6d8c40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=488, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0011169999570036632, \"learning_rate\": 1e-05, \"epoch\": 4.0, \"step\": 1952}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5878585b4a514fc9a102af7654dee919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-1952\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-1952/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.5232307195663453, \"eval_acc\": 0.9154228855721394, \"epoch\": 4.0, \"step\": 1952}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-1952/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd23539591744abe9bfb93f9c7c3ca82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=488, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
