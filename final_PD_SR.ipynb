{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    set_seed,\n",
    "    DataCollator\n",
    ")\n",
    "from typing import Callable, Dict, Optional, List\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "set_seed(37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVED_DIR_NAME = 'DP_EDA'\n",
    "PRETRAINED_MODEL = 'bert-base-uncased'\n",
    "NUM_LABELS = 2\n",
    "PAD_MAX_LEN = 65\n",
    "BATCH_SIZE = 128\n",
    "MAX_EPOCH = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/tidarren1020/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/tidarren1020/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/tidarren1020/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_jokes_path = './RedditHumorDetection/full_datasets/short_jokes/data/shortjokes.csv'\n",
    "# short_jokes_train_path = './RedditHumorDetection/data/short_jokes/train.tsv'\n",
    "# short_jokes_test_path = './RedditHumorDetection/data/short_jokes/test.tsv'\n",
    "# short_jokes_dev_path = './RedditHumorDetection/data/short_jokes/dev.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "puns_train_path = './RedditHumorDetection/data/puns/train.tsv'\n",
    "puns_test_path = './RedditHumorDetection/data/puns/test.tsv'\n",
    "puns_dev_path = './RedditHumorDetection/data/puns/dev.tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Paragraph Decmposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    df = pd.read_csv(data_path, header=None, names=['id','label','a','text'])\n",
    "    df = df[['label','text']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraph_decmposition(data_path):\n",
    "    df = load_data(data_path)\n",
    "    \n",
    "    dataAug = []\n",
    "    for _id,row in df.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['label']\n",
    "\n",
    "        tokens = text.split()\n",
    "\n",
    "        for i in range(2,len(tokens)):\n",
    "            text_a = ' '.join(tokens[:i])\n",
    "            text_b = ' '.join(tokens[i:])\n",
    "\n",
    "            d  = {'text_a':text_a, 'text_b':text_b, 'label':label, 'origin_id':_id}\n",
    "\n",
    "            dataAug.append(d)\n",
    "    \n",
    "    df_dataAug = pd.DataFrame(dataAug)\n",
    "    print('=== Data Augmentation: paragraph decmposition ===')\n",
    "    print('[Before]')\n",
    "    print('# of label=0:',sum(df.label==0))\n",
    "    print('# of label=1:',sum(df.label==1))\n",
    "    print('\\n[After]')\n",
    "    print('# of label=0:',sum(df_dataAug.label==0))\n",
    "    print('# of label=1:',sum(df_dataAug.label==1))\n",
    "    print('=== end ===')\n",
    "    return df_dataAug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumorDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, df, tokenizer, mode='train'):\n",
    "        self.df = df\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERT tokenizer\n",
    "        self.mode = mode\n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode=='train':\n",
    "            text_a = self.df.loc[idx, 'text_a']\n",
    "            text_b = self.df.loc[idx, 'text_b']\n",
    "            inputDict = tokenizer.encode_plus(text_a, text_b)\n",
    "        else:\n",
    "            text_a = self.df.loc[idx, 'text']\n",
    "            inputDict = tokenizer.encode_plus(text_a)\n",
    "        \n",
    "        label = self.df.loc[idx, 'label']\n",
    "        inputDict['label'] = label\n",
    "        \n",
    "        return inputDict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(seq, max_batch_len, pad_value):\n",
    "    return seq + (max_batch_len - len(seq)) * [pad_value]\n",
    "\n",
    "class Collator(DataCollator):\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def collate_batch(self, batch):\n",
    "        batch_inputs = list()\n",
    "        batch_attention_masks = list()\n",
    "        batch_token_type_ids = list()\n",
    "        labels = list()\n",
    "        max_size = max([len(ex['input_ids']) for ex in batch])\n",
    "        for item in batch:\n",
    "            batch_inputs += [pad_seq(item['input_ids'], max_size, self.pad_token_id)]\n",
    "            batch_attention_masks += [pad_seq(item['attention_mask'], max_size, 0)]\n",
    "            batch_token_type_ids += [pad_seq(item['token_type_ids'], max_size, 0)]\n",
    "            labels.append(item['label'])\n",
    "\n",
    "        return {\"input_ids\": torch.tensor(batch_inputs, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(batch_attention_masks, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(batch_token_type_ids, dtype=torch.long),\n",
    "                \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as naf\n",
    "\n",
    "from nlpaug.util import Action\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "an elevator makes ghosts happy because it lifts\n",
      "Augmented Text:\n",
      "an elevator makes ghosts happy because information technology lifts\n",
      "an elevator makes ghost happy because information technology lifts\n",
      "an elevator makes ghosts glad because information technology lifts\n",
      "an elevator makes ghosts felicitous because information technology lifts\n",
      "an lift have ghosts happy because it lifts\n"
     ]
    }
   ],
   "source": [
    "text = 'an elevator makes ghosts happy because it lifts'\n",
    "\n",
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "augmented_text = aug.augment(text, n=5)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "for t in augmented_text:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement():\n",
    "    aug = naw.SynonymAug(aug_src='wordnet')\n",
    "    data_synonym_replacement = []\n",
    "    df_puns_punchline = pd.read_csv('df_puns_punchline.csv')\n",
    "    for i,row in df_puns_punchline.iterrows():\n",
    "        text_a = row['text_a']\n",
    "        augmented_text = aug.augment(text_a, n=5)\n",
    "        d = {k:v for k,v in row.items()}\n",
    "        for text in augmented_text:\n",
    "            d_tmp = d.copy()\n",
    "            d_tmp['text_a'] = text\n",
    "            data_synonym_replacement.append(d_tmp)\n",
    "    df = pd.DataFrame(data_synonym_replacement)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_puns_sr = synonym_replacement()\n",
    "df_puns_sr = pd.read_csv('df_puns_sr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>origin_id</th>\n",
       "      <th>punchline_idx</th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>One m hoping they ll come and see this</td>\n",
       "      <td>and say We have to have this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>I m skip they ll come and see this</td>\n",
       "      <td>and say We have to have this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1 m hop skip they ll come and see this</td>\n",
       "      <td>and say We have to have this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>Iodin m hoping they ll come and see this</td>\n",
       "      <td>and say We have to have this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>I m hoping they ll come and take in this</td>\n",
       "      <td>and say We have to have this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>a man world health organization cannot take th...</td>\n",
       "      <td>illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>a piece world health organization cannot read ...</td>\n",
       "      <td>illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>a world world health organization cannot show ...</td>\n",
       "      <td>illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>a human race world health organization cannot ...</td>\n",
       "      <td>illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>a man who cannot scan the sign that warn peopl...</td>\n",
       "      <td>illiterate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>i fired the floor refinishers they simply coul...</td>\n",
       "      <td>their lacquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>single fired the floor refinishers they simply...</td>\n",
       "      <td>their lacquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>ace fired the floor preserver they simply coul...</td>\n",
       "      <td>their lacquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>i fired the flooring preserver they simply cou...</td>\n",
       "      <td>their lacquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>iodin fired the floor refinishers they simply ...</td>\n",
       "      <td>their lacquer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>an lift makes ghosts happy because information...</td>\n",
       "      <td>the spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>an elevator makes ghosts happy because it lift</td>\n",
       "      <td>the spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>an elevator makes specter happy because it lifts</td>\n",
       "      <td>the spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>an elevator makes ghostwriter happy because it...</td>\n",
       "      <td>the spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>an elevator makes ghosts happy because informa...</td>\n",
       "      <td>the spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>the world class drinking establishment in alaska</td>\n",
       "      <td>was a polar bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>the first drinking establishment in last frontier</td>\n",
       "      <td>was a polar bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>the 1st drinking establishment in alaska</td>\n",
       "      <td>was a polar bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>the foremost drinking establishment in alaska</td>\n",
       "      <td>was a polar bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>the first drinking validation in alaska</td>\n",
       "      <td>was a polar bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>They will pay a mary leontyne price for</td>\n",
       "      <td>it if they try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>They volition pay a price for</td>\n",
       "      <td>it if they try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>They will yield a price for</td>\n",
       "      <td>it if they try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>They leave pay a price for</td>\n",
       "      <td>it if they try</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>They will pay a cost for</td>\n",
       "      <td>it if they try</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  origin_id  punchline_idx  \\\n",
       "0       0          0              7   \n",
       "1       0          0              7   \n",
       "2       0          0              7   \n",
       "3       0          0              7   \n",
       "4       0          0              7   \n",
       "5       1          1             15   \n",
       "6       1          1             15   \n",
       "7       1          1             15   \n",
       "8       1          1             15   \n",
       "9       1          1             15   \n",
       "10      1          2              8   \n",
       "11      1          2              8   \n",
       "12      1          2              8   \n",
       "13      1          2              8   \n",
       "14      1          2              8   \n",
       "15      1          3              6   \n",
       "16      1          3              6   \n",
       "17      1          3              6   \n",
       "18      1          3              6   \n",
       "19      1          3              6   \n",
       "20      1          4              4   \n",
       "21      1          4              4   \n",
       "22      1          4              4   \n",
       "23      1          4              4   \n",
       "24      1          4              4   \n",
       "25      0          5              4   \n",
       "26      0          5              4   \n",
       "27      0          5              4   \n",
       "28      0          5              4   \n",
       "29      0          5              4   \n",
       "\n",
       "                                               text_a  \\\n",
       "0              One m hoping they ll come and see this   \n",
       "1                  I m skip they ll come and see this   \n",
       "2              1 m hop skip they ll come and see this   \n",
       "3            Iodin m hoping they ll come and see this   \n",
       "4            I m hoping they ll come and take in this   \n",
       "5   a man world health organization cannot take th...   \n",
       "6   a piece world health organization cannot read ...   \n",
       "7   a world world health organization cannot show ...   \n",
       "8   a human race world health organization cannot ...   \n",
       "9   a man who cannot scan the sign that warn peopl...   \n",
       "10  i fired the floor refinishers they simply coul...   \n",
       "11  single fired the floor refinishers they simply...   \n",
       "12  ace fired the floor preserver they simply coul...   \n",
       "13  i fired the flooring preserver they simply cou...   \n",
       "14  iodin fired the floor refinishers they simply ...   \n",
       "15  an lift makes ghosts happy because information...   \n",
       "16     an elevator makes ghosts happy because it lift   \n",
       "17   an elevator makes specter happy because it lifts   \n",
       "18  an elevator makes ghostwriter happy because it...   \n",
       "19  an elevator makes ghosts happy because informa...   \n",
       "20   the world class drinking establishment in alaska   \n",
       "21  the first drinking establishment in last frontier   \n",
       "22           the 1st drinking establishment in alaska   \n",
       "23      the foremost drinking establishment in alaska   \n",
       "24            the first drinking validation in alaska   \n",
       "25            They will pay a mary leontyne price for   \n",
       "26                      They volition pay a price for   \n",
       "27                        They will yield a price for   \n",
       "28                         They leave pay a price for   \n",
       "29                           They will pay a cost for   \n",
       "\n",
       "                          text_b  \n",
       "0   and say We have to have this  \n",
       "1   and say We have to have this  \n",
       "2   and say We have to have this  \n",
       "3   and say We have to have this  \n",
       "4   and say We have to have this  \n",
       "5                     illiterate  \n",
       "6                     illiterate  \n",
       "7                     illiterate  \n",
       "8                     illiterate  \n",
       "9                     illiterate  \n",
       "10                 their lacquer  \n",
       "11                 their lacquer  \n",
       "12                 their lacquer  \n",
       "13                 their lacquer  \n",
       "14                 their lacquer  \n",
       "15                   the spirits  \n",
       "16                   the spirits  \n",
       "17                   the spirits  \n",
       "18                   the spirits  \n",
       "19                   the spirits  \n",
       "20               was a polar bar  \n",
       "21               was a polar bar  \n",
       "22               was a polar bar  \n",
       "23               was a polar bar  \n",
       "24               was a polar bar  \n",
       "25                it if they try  \n",
       "26                it if they try  \n",
       "27                it if they try  \n",
       "28                it if they try  \n",
       "29                it if they try  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_puns_sr.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41681 entries, 0 to 41680\n",
      "Data columns (total 4 columns):\n",
      "label        41681 non-null int64\n",
      "origin_id    41681 non-null int64\n",
      "text_a       41681 non-null object\n",
      "text_b       41681 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_puns_pd = pd.read_csv('df_puns_pd.csv')\n",
    "df_puns_pd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58834 entries, 0 to 58833\n",
      "Data columns (total 5 columns):\n",
      "label            58834 non-null int64\n",
      "origin_id        58834 non-null int64\n",
      "text_a           58834 non-null object\n",
      "text_b           58834 non-null object\n",
      "punchline_idx    17153 non-null float64\n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_concat = pd.concat([df_puns_pd, df_puns_sr], sort=False).reset_index(drop=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>origin_id</th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>punchline_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I m</td>\n",
       "      <td>hoping they ll come and see this and say We ha...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I m hoping</td>\n",
       "      <td>they ll come and see this and say We have to h...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I m hoping they</td>\n",
       "      <td>ll come and see this and say We have to have this</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I m hoping they ll</td>\n",
       "      <td>come and see this and say We have to have this</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I m hoping they ll come</td>\n",
       "      <td>and see this and say We have to have this</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  origin_id                   text_a  \\\n",
       "0      0          0                      I m   \n",
       "1      0          0               I m hoping   \n",
       "2      0          0          I m hoping they   \n",
       "3      0          0       I m hoping they ll   \n",
       "4      0          0  I m hoping they ll come   \n",
       "\n",
       "                                              text_b  punchline_idx  \n",
       "0  hoping they ll come and see this and say We ha...            NaN  \n",
       "1  they ll come and see this and say We have to h...            NaN  \n",
       "2  ll come and see this and say We have to have this            NaN  \n",
       "3     come and see this and say We have to have this            NaN  \n",
       "4          and see this and say We have to have this            NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_puns_dev = load_data(puns_dev_path)\n",
    "\n",
    "train_dataset = HumorDataset(df_concat, tokenizer)\n",
    "eval_dataset = HumorDataset(df_puns_dev, tokenizer, mode='dev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"acc\": (preds == p.label_ids).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_TRAINSET = len(train_dataset)\n",
    "LOGGING_STEPS = math.ceil(NUM_TRAINSET/BATCH_SIZE)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/{}\".format(SAVED_DIR_NAME),\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=MAX_EPOCH,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    logging_first_step=True,\n",
    "    save_steps=LOGGING_STEPS,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    #learning_rate=2e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.training_args:PyTorch: setting up devices\n",
      "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=Collator(pad_token_id=tokenizer.pad_token_id),\n",
    "        compute_metrics=compute_metrics,\n",
    "        tb_writer=SummaryWriter(log_dir='logs', flush_secs=10),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running training *****\n",
      "INFO:transformers.trainer:  Num examples = 58834\n",
      "INFO:transformers.trainer:  Num Epochs = 5\n",
      "INFO:transformers.trainer:  Instantaneous batch size per device = 128\n",
      "INFO:transformers.trainer:  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "INFO:transformers.trainer:  Gradient Accumulation steps = 1\n",
      "INFO:transformers.trainer:  Total optimization steps = 2300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d08f408e0594b74a4aae47b79df9124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=5, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca940117d5f4fa5babe20817de0e6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=460, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0014368480962255728, \"learning_rate\": 4.9978260869565216e-05, \"epoch\": 0.002173913043478261, \"step\": 1}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf09f973a1f44a38dfd01cad9a41d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.6584379196166992, \"eval_acc\": 0.5920398009950248, \"epoch\": 0.002173913043478261, \"step\": 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.07724789623677245, \"learning_rate\": 4e-05, \"epoch\": 1.0, \"step\": 460}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a2fe0c46eb4cd0a777e19da5cd40cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-460\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-460/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.5746870756149292, \"eval_acc\": 0.8971807628524047, \"epoch\": 1.0, \"step\": 460}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-460/pytorch_model.bin\n",
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:201: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80e335f9db54053abb158344fd0e736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=460, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0040636494875640064, \"learning_rate\": 3e-05, \"epoch\": 2.0, \"step\": 920}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17aba3d5803e4a14b9f52d77c0ed91b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-920\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-920/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.5140550553798675, \"eval_acc\": 0.9237147595356551, \"epoch\": 2.0, \"step\": 920}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-920/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e48addf3c644b5595b2fdfb8e454c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=460, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.0015376685281808023, \"learning_rate\": 2e-05, \"epoch\": 3.0, \"step\": 1380}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "466ef1375a0f4572bdbab7481a1df1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-1380\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-1380/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.520938640832901, \"eval_acc\": 0.9286898839137645, \"epoch\": 3.0, \"step\": 1380}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-1380/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d30eaffcfe49db83ed1a6a6b74b7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=460, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.00019834798336966703, \"learning_rate\": 1e-05, \"epoch\": 4.0, \"step\": 1840}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc8aacdffac4077b0abb3bd8f78e45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-1840\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-1840/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.636886739730835, \"eval_acc\": 0.9220563847429519, \"epoch\": 4.0, \"step\": 1840}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-1840/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7d1ab418e346b69ed9b06ce8936603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=460, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.00018896343141497473, \"learning_rate\": 0.0, \"epoch\": 5.0, \"step\": 2300}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1012d9d9e244eca8dd62e7ca8e92a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./models/DP_EDA/checkpoint-2300\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./models/DP_EDA/checkpoint-2300/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"eval_loss\": 0.628925359249115, \"eval_acc\": 0.9203980099502488, \"epoch\": 5.0, \"step\": 2300}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./models/DP_EDA/checkpoint-2300/pytorch_model.bin\n",
      "INFO:transformers.trainer:\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2300, training_loss=0.016934674752705495)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:***** Running Evaluation *****\n",
      "INFO:transformers.trainer:  Num examples = 603\n",
      "INFO:transformers.trainer:  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3aa193c4f954fa8ba62037c79523d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluation', max=5, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\"eval_loss\": 0.628925359249115, \"eval_acc\": 0.9203980099502488, \"epoch\": 5.0, \"step\": 2300}\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
